# Objectif
√âtant donn√© un texte d'entr√©e, on veut le repr√©senter par une sequence num√©rique.
Un tokenizer a 2 fonctions
- l'encodeur ```tokenizer.encode(str) -> list(int)```
- le d√©codeur ```tokenizer.decode(list(int)) -> str```

# Impl√©mentations na√Øves

## Character level 
On associe √† chaque lettre un code.
```
ESPACE¬†-> 32
! -> 33
A -> 65
...
Z -> 90
[ -> 91
...
a -> 97
b -> 98
...


```
Exemple :  ```encode("salut toi !") -> [115, 97, 108, 117, 116, 32, 116, 111, 105, 32, 33]```
Cette impl√©mentation est simple, mais a des limitations:
- il faut d√©finir √† l'avance tout notre vocabulaire, il faut tous les caract√®res sp√©ciaux, mais il faut aussi penser aux autres langues. Les kanjis, les √©mojis.
- cela revient √† encoder l'information lettre par lettre, intuitivement on peut se dire que ce nn'est pas tr√®s efficace et qu'on pourrait travailler au niveau des mots. C'est √† dire associer √† un mot, un code.

## Word level
On associe un code √† chaque mot du dictionnaire (on suppose qu'il y en a 30.000)
```
a -> 1
abaca -> 2
abacule -> 3
...
zymotique -> 29.999
zythum -> 30.000
! -> 30.001
, -> 30.002
```
Exemple : ```encode("salut toi !") -> [21458,27551,30001]```
Analyse :
- on doit d'abord traiter le texte, pour le d√©couper en mot : il faut donc g√©rer les espaces, les tabulations
- on doit avoir un dictionnaire de tous les mots possibles √† encoder. Mais alors comment encoder les fautes j'√©cris ```maiston``` au lieu de ```maison```.

# Byte pair encoding
Il s'agit de l'encodage utilis√© dans GPT-4. On peut le tester avec la biblioth√®que tiktoken publi√©e par OpenAI
## Principe
L'encodage fonctionne au niveau de la repr√©sentation binaire du texte. Il s'appuie sur l'encodage UTF-8, du texte et va chercher √† compresser l'information en cherchant quelles sont les s√©quences les plus fr√©quentes.
√âtapes
- pre-processing : s√©parer le texte en mots
- encoder les mots en tokens. 
- construire une liste avec tous les tokens par concat√©nation
Propri√©t√©s :
- On ne veut pas que notre encodage d√©pende de la ponctuation. Il ne faut pas que les termes ```maison```, ```maison.```, ```maison?``` fassent changer l'encodage du mot ```maison```.
- on veut pouvoir encoder des mots qu‚Äôon n‚Äôa jamais vu
- on veut pouvoir choisir la taille de notre vocabulaire
### 1 Pre-processing
On commence par d√©couper le texte d‚Äôentr√©e en mots. Voici les expressions r√©guli√®res utilis√©es par GPT2 et GPT4

#### GPT2
```r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""```
- R1 : ``` ?\p{L}+``` : un espace optionnel suivi, d'une ou plusieurs lettres
- R2 : ``` ?\p{N}+``` : un espace optionnel suivi, d'une ou plusieurs chiffres
- R3 : ```'s``` : exactement la cha√Æne "'s"
- R4 √† R9 : idem avec 't 're 'ev 'm 'll et 'd
- R10:¬†``` ?[^\s\p{L}\p{N}]+```si les r√®gles suivantes ne s'appliquent pas, un espace optionnel suivi un caract√®re qui n'est pas ni un espace, ni une lettre, ni un chiffre -> la ponctuation
- R11 : ```\s+(?!\S)``` : une s√©rie d'espace sans prendre le dernier espace. Utile car dans le reste de la tokenization on a souvent <ESPACE>Token, donc on garde ce dernier espace pour le mot suivant.
- R12 : ```\s+``` : une suite de plusieurs espaces cons√©cutifs, les derniers espaces √† la fin de la phrase

#### GPT4
"pat_str": r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}++|\p{N}{1,3}+| ?[^\s\p{L}\p{N}]++[\r\n]*+|\s++$|\s*[\r\n]|\s+(?!\S)|\s""",
- ?i: - case insensitive
- \p{N}{1,3}+ :¬†les chiffres sont fusionn√©s par paquet de 3 maximum

### 2 Encodage
On part de la repr√©sentation de chaque caract√®re dans l‚Äôencodage UTF-8. On a pour un caract√®re un encodage entre 1 et 4 octets:

    a      U+0061   lettre a    01100001                               91
    [SP]   U+0020   espace      00100000                               20
    √©      U+00E9   e accentu√©  11000011 10101001                      195 169
    „ÅÅ     U+3041   Hiragana    11100011 10000001 10000001             227 129 129 
    üòÖ    U+1F605  smiley      11110000 10011111 10011000 10000101    240 159 152 133

On va prendre un des mots de l‚Äô√©tape 1, et le convertir dans la repr√©sentation d√©cimale de son encodage UTF-8.
Le mot ```Maison``` devient ```[77,97,105,115,111,110]```. √Ä ce stade le mot ```Maison``` est repr√©sent√© par 6 tokens. Et on a chaque token qui est un chiffre entre 0 et 255.
On fait cela pour tous les mots de notre dataset.

### 3 Appariement et cr√©ation de token
On va ensuite chercher la paire de chiffre la plus fr√©quente. Si on suppose que dans notre jeu de donn√©es c‚Äôest ```le``` -> ```[108,101]```, alors on cr√©er un nouveau token le num√©ro 256 qui va repr√©senter ```le```.
On a maintenanant un vocabulaire de 257 tokens (0 √† 256), on remplace tous les occurrences de la paire ```[108,101]```par ce token ```256```, et on recommence.
Recherche de la paire la plus fr√©quente, cr√©ation du token ```257``` et remplacement de cette paire par le nouveau token.
On continue jusqu‚Äô√† soit qu‚Äôil n‚Äôy ait plas de paire, soit qu‚Äôon ait atteint le vocabulaire max qu‚Äôon s‚Äôest fix√©.

Exemples issus de GPT:
- "ine" est le token 500
- "ice" est le token 501
- "against" est le token 32826
- " against" est le token 1028

Note: Sentence piece, essaie de compenser le fait que deux fois le meme mot avec un espace devant ont un encodage diff√©rent en ajoutant un espace au d√©but de la phrase.

### Special tokens
dans GPT2, encoder('<|endoftext|>) = 50256, c'est le dernier token encod√©. Il sert √† d√©limiter les documents. Cela permet au mod√®le d'apprendre qu'il doit repartir √† z√©ro quand il voit ce token
Dans le fine-tuning on utilise aussi des tokens sp√©ciaux comme
```<|im_start|>system<|im_sep|>You are a helpful assistant<|im_end|><|im_start|>user<|im_sep|><|im_end|><|im_start|>assistant<|im_sep|>```
Ici ces tokens servent √† s√©parer les √©l√©ments du dialogue:
- im_start :¬†imaginary dialog start

Le traitement de ces tokens est fait dans l'algorithme par des traitements sp√©ciaux. Ce n'est pas BPE qui g√®re ces cas.
FIM :¬†fill in the middle

Il est possible d'√©tendre le vocabulaire et de rajouter des specials tokens. 
- dans la librairie tiktoken c'est pr√©vu
- attention cela a un impact sur l'architecture de transformer. Il faut rajouter une ligne dans les embeddings, et rajouter une sortie dans l'output layer car on a un token de plus dans la liste des probabilit√©s

# Impact sur l‚Äôarchitecture du LLM
Le nombre de tokens qu‚Äôon a c‚Äôest √† dire la taille de notre vocabulaire ```vocab_size``` a un impact sur l‚Äôarchitecture du LLM. Si on prend l‚Äôexemple de GPT-2, on a un ```vocab_size=50256```
Ce nombre ```50256``` se retrouve dans deux endroits :
- la table d‚Äôembedding
- le neurone de sortie
## Embedding table
```token_embedding_table = nn. Embeddding(vocab_size, n_embd) : ```
- 2-dim array. 
- nombre de lignes est notre vocabulaire, chaque token est associ√© √† un vecteur d‚Äôembedding qu'on entraine durant la backpropagation,
- n_embd : nbre de channels dans notre transformer

## Neurone de sortie
```lm_head = nn.Linear(n_embd, vocab_size)```
- linear layer
- produce logits
- probability of every single tokens
Le neurone de sortie donne pour chacun des tokens du vocabulaire qu‚Äôelle est la probabilit√© qu‚Äôil soit le prochain. 

## Taille du vocabulaire
Plus on a de tokens, plus la repr√©sentation du texte est dense. Or il faut consid√©rer le fait que dans l‚Äôarchitecture de transformer on a un contexte qui est de taille fixe. Si on a un contexte de 1024, cela veut dire que le LLM‚ÄØva pouvoir manipuler 1024 tokens √† la fois.  
Si on augmente trop le nombre de tokens, par exemple on dit qu'on a un vocab_size = 1 million. 
- Alors dans les donn√©es d'entrainement on aura des tokens tr√®s rares, et l'entrainement ne pas voir beaucoup de donn√©es sur certains tokens ce qui va faire que les embeddings seront sous-entrain√©. Les tokens vont √™tre mal repr√©sent√©s dans notre espace vectoriel d‚Äôembedding.
- on va compresser √©norm√©ment d'info dans un seul token (on aura des tokens tr√®s longs), et lors de la forward pass on ne va pas bien entrainer le r√©seau. Le r√©seau ne pourra pas ajuster les poids de fa√ßon optimale

### ajouter un token lors du fine-tuning
On doit cr√©er un embedding initiali√© au hasard pour ce nouveau token
On doit aussi changer le dernier neurone.
On refait un training avec ces tokens

# Limitations d√ªes au tokenizer
- spelling :¬†les mots sont transform√©s en token qui peuvent √™tre tr√®s long. Donc le LLM ne sait pas d√©composer un mot
- ex dans gpt4 il y a token  .DefaultCellStyle -> c'est vu comme un seul token.
- reverse string. idem si on lui demande de l'√©peller √† l'envers. Il faut lui dire de d√©composer les lettres, et ensuite il a les tokens individuels
- gestion des langues √©trang√®res : il a vu moins d'info dans le training du mod√®le, mais aussi durant la tokenization o√π on occupe beaucoup plus de tokens. Hello how are you ? -> 5 tokens, en cor√©en c'est 15 !
- arithm√©tique : les chiffres/nombres sont tokeniz√©s tr√®s diff√©remment, on peut avoir 123 en 1 token mais 778 c'est 2 tokens. Or l'addition va devoir d√©composer les unit√©s, les dizaines, les centaines.
- si on demande de compl√©ter alors qu'on finit notre phrase par un espace, on risque de tomber sur des cas tr√®s peu probables. En effet les tokens commencent par un espace. Or l'espace a d√©j√† √©t√© mis, le comportement risque d'√™tre assez al√©atoire. M√™me probl√®me si on veut compl√©ter un token tr√®s long.
.DefaultCellSty -> vu comme [13578,3683,626,88] il va essayer de trouver un nouveau token alors que la r√©ponse c'est un token unique [86662]
- solidmagickarp : token cr√©√© durant le training du tokenizer. Il est apparu suffisament pour avoir son token. Mais si pas de donn√©es dans le LLM avec ce token, alors l'embedding associ√© n'est jamais entrain√© et les poids associ√©s restent ceux initialis√©s au hasard. Le mod√®le va donc g√©n√©rer n'importe quoi.
