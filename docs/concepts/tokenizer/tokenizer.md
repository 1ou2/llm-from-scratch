# Objectif
Ã‰tant donnÃ© un texte d'entrÃ©e, on veut le reprÃ©senter par une sequence numÃ©rique.
Un tokenizer a 2 fonctions
- l'encodeur ```tokenizer.encode(str) -> list(int)```
- le dÃ©codeur ```tokenizer.decode(list(int)) -> str```

Exemple avec le tokenizer dâ€™Openai qui sâ€™appelle ```tiktoken```.
```python
text = "hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ğŸ˜‰"

# tiktoken
import tiktoken
# use GPT-4 encoder
enc = tiktoken.get_encoding("cl100k_base")
print(enc.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

text = "<|endoftext|>hello world"
enc = tiktoken.get_encoding("cl100k_base")
print(enc.encode(text, allowed_special="all"))
# [100257, 15339, 1917]
```

[![](./images/LLM-tokens.png)](./images/LLM-tokens.png)

# ImplÃ©mentations naÃ¯ves

## Character level 
On associe Ã  chaque lettre un code.
```
ESPACEÂ -> 32
! -> 33
A -> 65
...
Z -> 90
[ -> 91
...
a -> 97
b -> 98
...


```
Exemple :  ```encode("salut toi !") -> [115, 97, 108, 117, 116, 32, 116, 111, 105, 32, 33]```
Cette implÃ©mentation est simple, mais a des limitations:
- il faut dÃ©finir Ã  l'avance tout notre vocabulaire, il faut tous les caractÃ¨res spÃ©ciaux, mais il faut aussi penser aux autres langues. Les kanjis, les Ã©mojis.
- cela revient Ã  encoder l'information lettre par lettre, intuitivement on peut se dire que ce nn'est pas trÃ¨s efficace et qu'on pourrait travailler au niveau des mots. C'est Ã  dire associer Ã  un mot, un code.

## Word level
On associe un code Ã  chaque mot du dictionnaire (on suppose qu'il y en a 30.000)
```
a -> 1
abaca -> 2
abacule -> 3
...
zymotique -> 29.999
zythum -> 30.000
! -> 30.001
, -> 30.002
```
Exemple : ```encode("salut toi !") -> [21458,27551,30001]```

Analyse :
- on doit d'abord traiter le texte, pour le dÃ©couper en mot : il faut donc gÃ©rer les espaces, les tabulations
- on doit avoir un dictionnaire de tous les mots possibles Ã  encoder. Mais alors comment encoder les fautes j'Ã©cris ```maiston``` au lieu de ```maison```. Comment gÃ©rer les langues qu'on a pas vu (javanais, arabe, ...)

# Byte pair encoding
Il s'agit de l'encodage utilisÃ© dans GPT-4. On peut le tester avec la bibliothÃ¨que tiktoken publiÃ©e par OpenAI
## Principe
L'encodage fonctionne au niveau de la reprÃ©sentation binaire du texte. Il s'appuie sur l'encodage UTF-8, du texte et va chercher Ã  compresser l'information en cherchant quelles sont les sÃ©quences les plus frÃ©quentes.

Ã‰tapes :

- pre-processing : sÃ©parer le texte en mots
- encoder les mots en tokens
- gÃ©nÃ©rer des tokens pour les paires les plus frÃ©quentes

PropriÃ©tÃ©s :

- On ne veut pas que notre encodage dÃ©pende de la ponctuation. Il ne faut pas que les termes ```maison```, ```maison.```, ```maison?``` fassent changer l'encodage du mot ```maison```.
- on veut pouvoir encoder des mots quâ€™on nâ€™a jamais vu
- on veut pouvoir choisir la taille de notre vocabulaire

[![](./images/tokens.png)](./images/tokens.png)
### 1/ Pre-processing
On commence par dÃ©couper le texte dâ€™entrÃ©e en mots. Voici les expressions rÃ©guliÃ¨res utilisÃ©es par GPT2 et GPT4

#### GPT2
```r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""```
- R1 : ``` ?\p{L}+``` : un espace optionnel suivi, d'une ou plusieurs lettres
- R2 : ``` ?\p{N}+``` : un espace optionnel suivi, d'une ou plusieurs chiffres
- R3 : ```'s``` : exactement la chaÃ®ne "'s"
- R4 Ã  R9 : idem avec 't 're 'ev 'm 'll et 'd
- R10:Â ``` ?[^\s\p{L}\p{N}]+```si les rÃ¨gles suivantes ne s'appliquent pas, un espace optionnel suivi un caractÃ¨re qui n'est pas ni un espace, ni une lettre, ni un chiffre -> la ponctuation
- R11 : ```\s+(?!\S)``` : une sÃ©rie d'espace sans prendre le dernier espace. Utile car dans le reste de la tokenization on a souvent <ESPACE>Token, donc on garde ce dernier espace pour le mot suivant.
- R12 : ```\s+``` : une suite de plusieurs espaces consÃ©cutifs, les derniers espaces Ã  la fin de la phrase

#### GPT4
```r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}++|\p{N}{1,3}+| ?[^\s\p{L}\p{N}]++[\r\n]*+|\s++$|\s*[\r\n]|\s+(?!\S)|\s"""```
- ?i: - case insensitive
- \p{N}{1,3}+ :Â les chiffres sont fusionnÃ©s par paquet de 3 maximum

### 2/ Encodage
On part de la reprÃ©sentation de chaque caractÃ¨re dans lâ€™encodage UTF-8. On a pour un caractÃ¨re un encodage entre 1 et 4 octets:

    a      U+0061   lettre a    01100001                               91
    [SP]   U+0020   espace      00100000                               20
    Ã©      U+00E9   e accentuÃ©  11000011 10101001                      195 169
    ã     U+3041   Hiragana    11100011 10000001 10000001             227 129 129 
    ğŸ˜…    U+1F605  smiley      11110000 10011111 10011000 10000101    240 159 152 133

On va prendre un des mots de lâ€™Ã©tape 1, et le convertir dans la reprÃ©sentation dÃ©cimale de son encodage UTF-8.
Le mot ```Maison``` devient ```[77,97,105,115,111,110]```. Ã€ ce stade le mot ```Maison``` est reprÃ©sentÃ© par 6 tokens. Et on a chaque token qui est un chiffre entre 0 et 255.
On fait cela pour tous les mots de notre dataset.

### 3/ Appariement et crÃ©ation de token
On va ensuite chercher la paire de chiffre la plus frÃ©quente. Si on suppose que dans notre jeu de donnÃ©es câ€™est ```le``` -> ```[108,101]```, alors on crÃ©er un nouveau token le numÃ©ro 256 qui va reprÃ©senter ```le```.
On a maintenanant un vocabulaire de 257 tokens (0 Ã  256), on remplace tous les occurrences de la paire ```[108,101]```par ce token ```256```, et on recommence.
Recherche de la paire la plus frÃ©quente, crÃ©ation du token ```257``` et remplacement de cette paire par le nouveau token.
On continue jusquâ€™Ã  soit quâ€™il nâ€™y ait plas de paire, soit quâ€™on ait atteint le vocabulaire max quâ€™on sâ€™est fixÃ©.

Exemples issus de GPT:
- "ine" est le token 500
- "ice" est le token 501
- "against" est le token 32826
- " against" est le token 1028

Note: Sentence piece, essaie de compenser le fait que deux fois le meme mot avec un espace devant ont un encodage diffÃ©rent en ajoutant un espace au dÃ©but de la phrase.

### Special tokens
dans GPT2, encoder('<|endoftext|>) = 50256, c'est le dernier token encodÃ©. Il sert Ã  dÃ©limiter les documents. Cela permet au modÃ¨le d'apprendre qu'il doit repartir Ã  zÃ©ro quand il voit ce token
Dans le fine-tuning on utilise aussi des tokens spÃ©ciaux comme
```<|im_start|>system<|im_sep|>You are a helpful assistant<|im_end|><|im_start|>user<|im_sep|><|im_end|><|im_start|>assistant<|im_sep|>```
Ici ces tokens servent Ã  sÃ©parer les Ã©lÃ©ments du dialogue:
- im_start :Â imaginary dialog start

Le traitement de ces tokens est fait dans l'algorithme par des traitements spÃ©ciaux. Ce n'est pas BPE qui gÃ¨re ces cas.
FIM :Â fill in the middle

Il est possible d'Ã©tendre le vocabulaire et de rajouter des specials tokens. 
- dans la librairie tiktoken c'est prÃ©vu
- attention cela a un impact sur l'architecture de transformer. Il faut rajouter une ligne dans les embeddings, et rajouter une sortie dans l'output layer car on a un token de plus dans la liste des probabilitÃ©s

# Impact sur lâ€™architecture du LLM
Le nombre de tokens quâ€™on a câ€™est Ã  dire la taille de notre vocabulaire ```vocab_size``` a un impact sur lâ€™architecture du LLM. Si on prend lâ€™exemple de GPT-2, on a un ```vocab_size=50256```
Ce nombre ```50256``` se retrouve dans deux endroits :
- la table dâ€™embedding
- le neurone de sortie
## Embedding table
```token_embedding_table = nn. Embeddding(vocab_size, n_embd) : ```
- 2-dim array. 
- nombre de lignes est notre vocabulaire, chaque token est associÃ© Ã  un vecteur dâ€™embedding qu'on entraine durant la backpropagation,
- n_embd : nbre de channels dans notre transformer

## Neurone de sortie
```lm_head = nn.Linear(n_embd, vocab_size)```
- linear layer
- produce logits
- probability of every single tokens
Le neurone de sortie donne pour chacun des tokens du vocabulaire quâ€™elle est la probabilitÃ© quâ€™il soit le prochain. 

## Taille du vocabulaire
Plus on a de tokens, plus la reprÃ©sentation du texte est dense. Or il faut considÃ©rer le fait que dans lâ€™architecture de transformer on a un contexte qui est de taille fixe. Si on a un contexte de 1024, cela veut dire que le LLMâ€¯va pouvoir manipuler 1024 tokens Ã  la fois.  
Si on augmente trop le nombre de tokens, par exemple on dit qu'on a un vocab_size = 1 million. 
- Alors dans les donnÃ©es d'entrainement on aura des tokens trÃ¨s rares, et l'entrainement ne pas voir beaucoup de donnÃ©es sur certains tokens ce qui va faire que les embeddings seront sous-entrainÃ©. Les tokens vont Ãªtre mal reprÃ©sentÃ©s dans notre espace vectoriel dâ€™embedding.
- on va compresser Ã©normÃ©ment d'info dans un seul token (on aura des tokens trÃ¨s longs), et lors de la forward pass on ne va pas bien entrainer le rÃ©seau. Le rÃ©seau ne pourra pas ajuster les poids de faÃ§on optimale

### ajouter un token lors du fine-tuning
On doit crÃ©er un embedding initialiÃ© au hasard pour ce nouveau token
On doit aussi changer le dernier neurone.
On refait un training avec ces tokens

# Limitations dÃ»es au tokenizer
- spelling :Â les mots sont transformÃ©s en token qui peuvent Ãªtre trÃ¨s long. Donc le LLM ne sait pas dÃ©composer un mot
- ex dans gpt4 il y a token  .DefaultCellStyle -> c'est vu comme un seul token.
- reverse string. idem si on lui demande de l'Ã©peller Ã  l'envers. Il faut lui dire de dÃ©composer les lettres, et ensuite il a les tokens individuels
- gestion des langues Ã©trangÃ¨res : il a vu moins d'info dans le training du modÃ¨le, mais aussi durant la tokenization oÃ¹ on occupe beaucoup plus de tokens. Hello how are you ? -> 5 tokens, en corÃ©en c'est 15 !
- arithmÃ©tique : les chiffres/nombres sont tokenizÃ©s trÃ¨s diffÃ©remment, on peut avoir 123 en 1 token mais 778 c'est 2 tokens. Or l'addition va devoir dÃ©composer les unitÃ©s, les dizaines, les centaines.
- si on demande de complÃ©ter alors qu'on finit notre phrase par un espace, on risque de tomber sur des cas trÃ¨s peu probables. En effet les tokens commencent par un espace. Or l'espace a dÃ©jÃ  Ã©tÃ© mis, le comportement risque d'Ãªtre assez alÃ©atoire. MÃªme problÃ¨me si on veut complÃ©ter un token trÃ¨s long.
.DefaultCellSty -> vu comme [13578,3683,626,88] il va essayer de trouver un nouveau token alors que la rÃ©ponse c'est un token unique [86662]
- solidmagickarp : token crÃ©Ã© durant le training du tokenizer. Il est apparu suffisament pour avoir son token. Mais si pas de donnÃ©es dans le LLM avec ce token, alors l'embedding associÃ© n'est jamais entrainÃ© et les poids associÃ©s restent ceux initialisÃ©s au hasard. Le modÃ¨le va donc gÃ©nÃ©rer n'importe quoi.
