[model]
vocab_size = 32768
context_length = 1024
embed_dim = 768
n_layers = 12
n_heads = 12
drop_rate = 0.1
qkv_bias = False
tokenizer = custom

[hypers]
epochs = 1
lr = 6e-4
beta1 = 0.9
beta2 = 0.95
eps = 1e-8
weight_decay = 0.1
grad_clip = 1.0


[training]
tokens_per_shard = 1048577
n_shards = 1917
batch_size = 524288
micro_batch_size = 2
max_steps = 32
validation_steps = 2
num_generation_sequences = 1
use_checkpoint = True
log_interval = 1
gen_text_interval = 5
eval_interval = 4
checkpoint_interval = 2
use_compile = False


[files]
log_dir = logs/
log_file = logs.txt
stat_file = stats.txt
checkpoint_dir = checkpoints/
token_dir = data/shards/
tokenizer_dir = data/tokenizer/