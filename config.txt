[model]
vocab_size = 32768
context_length = 1024
embed_dim = 768
n_layers = 12
n_heads = 12
drop_rate = 0.1
qkv_bias = False
tokenizer = custom

[hypers]
epochs = 1
lr = 6e-4
beta1 = 0.9
beta2 = 0.95
eps = 1e-8
weight_decay = 0.1
grad_clip = 1.0
tokens_per_shard = 1000000

[training]
batch_size = 2
max_steps = 5000
validation_steps = 2
num_generation_sequences = 1
use_checkpoint = True
log_interval = 200
gen_text_interval = 500
eval_interval = 1000
checkpoint_interval = 2000
use_compile = False

[files]
log_dir = logs/
log_file = logs.txt
stat_file = stats.txt
checkpoint_dir = checkpoints/
token_dir = data/tokenized/gabwikifr/
tokenizer_dir = data/tokenizer/